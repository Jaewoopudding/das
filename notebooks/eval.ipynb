{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fairscale'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m aesthetic_fn \u001b[38;5;241m=\u001b[39m rewards\u001b[38;5;241m.\u001b[39maesthetic_score(torch_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# hps_fn = rewards.hps_score(inference_dtype = torch.float32, device = 'cuda')\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m imagereward \u001b[38;5;241m=\u001b[39m \u001b[43mrewards\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageReward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pick_fn \u001b[38;5;241m=\u001b[39m rewards\u001b[38;5;241m.\u001b[39mPickScore(inference_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m clip_fn \u001b[38;5;241m=\u001b[39m rewards\u001b[38;5;241m.\u001b[39mclip_score(inference_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32, device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/HDD/jaewoo/research/diffusion-mcts/DAS/das/rewards.py:138\u001b[0m, in \u001b[0;36mImageReward\u001b[0;34m(inference_dtype, device, return_loss)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mImageReward\u001b[39m(\n\u001b[1;32m    134\u001b[0m     inference_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    135\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m    136\u001b[0m     return_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m    137\u001b[0m ):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageReward_scorer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageRewardScorer\n\u001b[1;32m    140\u001b[0m     scorer \u001b[38;5;241m=\u001b[39m ImageRewardScorer(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    141\u001b[0m     scorer\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/HDD/jaewoo/research/diffusion-mcts/DAS/das/scorers/ImageReward_scorer.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPProcessor\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mImageReward\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBLIP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblip_pretrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BLIP_Pretrain\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mImageReward\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageReward_download\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMLP\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mReFL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/utils.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Union, List\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImageReward\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageReward\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/ImageReward.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBLIP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblip_pretrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BLIP_Pretrain\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Compose, Resize, CenterCrop, ToTensor, Normalize\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/models/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAestheticScore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBLIPScore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCLIPScore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBLIP\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/models/BLIPScore.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mImageReward\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBLIP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_checkpoint\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mImageReward\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBLIP\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblip_pretrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BLIP_Pretrain\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Compose, Resize, CenterCrop, ToTensor, Normalize\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/models/BLIP/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblip_pretrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/models/BLIP/blip_pretrain.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertConfig, BertModel\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mblip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_vit, init_tokenizer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mBLIP_Pretrain\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,                 \n\u001b[1;32m     15\u001b[0m                  med_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmed_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     16\u001b[0m                  image_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m224\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m                  momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.995\u001b[39m,\n\u001b[1;32m     23\u001b[0m                  ):\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/models/BLIP/blip.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_cached_file\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VisionTransformer, interpolate_pos_embed\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minit_tokenizer\u001b[39m():\n\u001b[1;32m     17\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/das/lib/python3.10/site-packages/ImageReward/models/BLIP/vit.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m trunc_normal_, DropPath\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m named_apply, adapt_input_conv\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfairscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint_activations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m checkpoint_wrapper\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMlp\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fairscale'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import das.rewards as rewards\n",
    "import csv\n",
    "\n",
    "aesthetic_fn = rewards.aesthetic_score(torch_dtype = torch.float32, device = 'cuda')\n",
    "hps_fn = rewards.hps_score(inference_dtype = torch.float32, device = 'cuda')\n",
    "imagereward = rewards.ImageReward(inference_dtype = torch.float32, device = 'cuda')\n",
    "pick_fn = rewards.PickScore(inference_dtype = torch.float32, device = 'cuda')\n",
    "clip_fn = rewards.clip_score(inference_dtype = torch.float32, device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/jovyan/conda/dfs/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "import csv\n",
    "import lpips\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the CLIP model and processor (using openai/clip-vit-large-patch14)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='alex')\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "lpips_model = lpips_model.to(device)\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "\n",
    "# Function to preprocess image for LPIPS\n",
    "def preprocess_image_lpips(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Function to calculate CLIP-based metrics and LPIPS\n",
    "def calculate_metrics(image_folder, K=20):\n",
    "    image_folder = os.path.join(image_folder, \"eval_vis\")\n",
    "    embeddings = []\n",
    "    lpips_images = []\n",
    "    image_files = [os.path.join(image_folder, file) for file in os.listdir(image_folder) if (file.endswith(('png', 'jpg', 'jpeg')) and not \"ess\" in file and not \"intermediate_rewards\" in file)]\n",
    "\n",
    "    if len(image_files) == 0:\n",
    "        raise ValueError(f\"No images found in the folder: {image_folder}\")\n",
    "\n",
    "    # Preprocess images and compute embeddings\n",
    "    for image_path in tqdm(image_files):\n",
    "        try:\n",
    "            # For CLIP\n",
    "            pixel_values = preprocess_image(image_path).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = model.get_image_features(pixel_values).cpu().numpy().squeeze()\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "            # For LPIPS\n",
    "            lpips_image = preprocess_image_lpips(image_path).to(device)\n",
    "            lpips_images.append(lpips_image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"No embeddings were generated. Please check your images and preprocessing steps.\")\n",
    "    \n",
    "    # ---- Calculate Mean Pairwise Distance (CLIP-based) ----\n",
    "    pairwise_distances = pdist(embeddings, metric='cosine')\n",
    "    mean_distance = np.mean(pairwise_distances)\n",
    "    num_distances = pairwise_distances.size\n",
    "    std_error = np.std(pairwise_distances) / np.sqrt(num_distances)\n",
    "    \n",
    "    # ---- Calculate Truncated CLIP Entropy (TCE) ----\n",
    "    covariance_matrix = np.cov(embeddings, rowvar=False)\n",
    "    eigenvalues = np.linalg.eigvalsh(covariance_matrix)[-K:]\n",
    "    TCE_K = (K / 2) * np.log(2 * np.pi * np.e) + (1 / 2) * np.sum(np.log(eigenvalues))\n",
    "    \n",
    "    # ---- Calculate LPIPS-based diversity ----\n",
    "    lpips_distances = []\n",
    "    num_images = len(lpips_images)\n",
    "    for i in range(num_images):\n",
    "        for j in range(i+1, num_images):\n",
    "            with torch.no_grad():\n",
    "                distance = lpips_model(lpips_images[i], lpips_images[j]).item()\n",
    "            lpips_distances.append(distance)\n",
    "    \n",
    "    mean_lpips = np.mean(lpips_distances)\n",
    "    std_lpips = np.std(lpips_distances)\n",
    "    \n",
    "    return mean_distance, std_error, TCE_K, mean_lpips, std_lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = \"logs/SMC/aesthetic/2024.09.26_01.12.19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating images in logs/SMC/aesthetic/2024.09.26_01.12.19\n",
      "Aesthetic score:  7.056802809238434\n",
      "Aesthetic score std:  0.3690912412318431\n",
      "HPS score:  0.27003603242337704\n",
      "HPS score std:  0.008796761060747487\n",
      "Image reward score:  1.1079169440781698\n",
      "Image reward score std:  0.5541496030510243\n",
      "Pick score:  0.21461675222963095\n",
      "Pick score std:  0.00884714490436561\n",
      "Clip score:  0.25687733199447393\n",
      "Clip score std:  0.019748996170760567\n"
     ]
    }
   ],
   "source": [
    "aesthetic_score = []\n",
    "hps_score = []\n",
    "imagereward_score = []\n",
    "pick_score = []\n",
    "clip_score = []\n",
    "image_names = [file for file in os.listdir(img_folder + \"/eval_vis\") if (file.endswith(('png', 'jpg', 'jpeg')) and not \"ess\" in file and not \"intermediate_rewards\" in file)]\n",
    "for image_name in image_names:\n",
    "\n",
    "    image_path = os.path.join(img_folder + \"/eval_vis\", image_name)\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = torchvision.transforms.ToTensor()(image).unsqueeze(0).to('cuda')\n",
    "\n",
    "    prompt = image_name.split(\"|\")[0].split(\"_\")[-1][:-1]\n",
    "    # print(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        clip_score.append(clip_fn(image, prompt).item())\n",
    "        aesthetic_score.append(aesthetic_fn(image, prompt).item())\n",
    "        hps_score.append(hps_fn(image, prompt).item())\n",
    "        imagereward_score.append(imagereward(image, prompt).item())\n",
    "        pick_score.append(pick_fn(image, prompt).item())\n",
    "\n",
    "print(f\"Finished evaluating images in {img_folder}\")\n",
    "print(\"Aesthetic score: \", np.mean(aesthetic_score))\n",
    "print(\"Aesthetic score std: \", np.std(aesthetic_score))\n",
    "print(\"HPS score: \", np.mean(hps_score))\n",
    "print(\"HPS score std: \", np.std(hps_score))\n",
    "print(\"Image reward score: \", np.mean(imagereward_score))\n",
    "print(\"Image reward score std: \", np.std(imagereward_score))\n",
    "print(\"Pick score: \", np.mean(pick_score))\n",
    "print(\"Pick score std: \", np.std(pick_score))\n",
    "print(\"Clip score: \", np.mean(clip_score))\n",
    "print(\"Clip score std: \", np.std(clip_score))\n",
    "    \n",
    "# Save the results to a text file\n",
    "names = [\"Aesthetic score\", \"Aesthetic score std\", \"HPS score\", \"HPS score std\",\n",
    "         \"Image reward score\", \"Image reward score std\", \"Pick score\", \"Pick score std\", \"CLIP score\", \"CLIP score std\"]\n",
    "\n",
    "values = [np.mean(aesthetic_score), np.std(aesthetic_score),\n",
    "          np.mean(hps_score), np.std(hps_score),\n",
    "          np.mean(imagereward_score), np.std(imagereward_score),\n",
    "          np.mean(pick_score), np.std(pick_score),\n",
    "          np.mean(clip_score), np.std(clip_score)]\n",
    "\n",
    "# Format the values to 5 decimal places\n",
    "formatted_values = [f\"{v:.5f}\" for v in values]\n",
    "\n",
    "with open(os.path.join(img_folder, \"eval_results.csv\"), \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(names)\n",
    "    writer.writerow(formatted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating images in logs/SMC/aesthetic/2024.09.26_01.12.19\n",
      "Mean Pairwise Distance (CLIP-based Diversity Metric): 0.33313817014833663\n",
      "Standard Error of the Distance: 0.007745194941195248\n",
      "Truncated CLIP Entropy (TCE): -42.314139664497574\n",
      "Mean LPIPS Distance: 0.647677265604337\n",
      "Standard Deviation of LPIPS Distance: 0.04706209534226535\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "try:\n",
    "    mean_distance, std_error, TCE, mean_lpips, std_lpips = calculate_metrics(img_folder, K=20)\n",
    "    print(f\"Finished evaluating images in {img_folder}\")\n",
    "    print(f\"Mean Pairwise Distance (CLIP-based Diversity Metric): {mean_distance}\")\n",
    "    print(f\"Standard Error of the Distance: {std_error}\")\n",
    "    print(f\"Truncated CLIP Entropy (TCE): {TCE}\")\n",
    "    print(f\"Mean LPIPS Distance: {mean_lpips}\")\n",
    "    print(f\"Standard Deviation of LPIPS Distance: {std_lpips}\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    names = [\"Mean Pairwise Distance (CLIP)\", \"Standard Error of the Distance (CLIP)\", \n",
    "             \"Truncated CLIP Entropy (TCE)\", \"Mean LPIPS Distance\", \"Std Dev LPIPS Distance\"]\n",
    "    values = [mean_distance, std_error, TCE, mean_lpips, std_lpips]\n",
    "\n",
    "    # Format the values to 5 decimal places\n",
    "    formatted_values = [f\"{v:.5f}\" for v in values]\n",
    "\n",
    "    with open(os.path.join(img_folder, \"eval_diversity_results.csv\"), \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(names)\n",
    "        writer.writerow(formatted_values)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "das",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
