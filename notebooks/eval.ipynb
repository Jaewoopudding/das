{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/conda/dfs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jovyan/conda/dfs/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import das.rewards as rewards\n",
    "import csv\n",
    "\n",
    "aesthetic_fn = rewards.aesthetic_score(torch_dtype = torch.float32, device = 'cuda')\n",
    "hps_fn = rewards.hps_score(inference_dtype = torch.float32, device = 'cuda')\n",
    "imagereward = rewards.ImageReward(inference_dtype = torch.float32, device = 'cuda')\n",
    "pick_fn = rewards.PickScore(inference_dtype = torch.float32, device = 'cuda')\n",
    "clip_fn = rewards.clip_score(inference_dtype = torch.float32, device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/jovyan/conda/dfs/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "import csv\n",
    "import lpips\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the CLIP model and processor (using openai/clip-vit-large-patch14)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "\n",
    "# Load LPIPS model\n",
    "lpips_model = lpips.LPIPS(net='alex')\n",
    "\n",
    "# Device configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "lpips_model = lpips_model.to(device)\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return processor(images=image, return_tensors=\"pt\")['pixel_values'].squeeze(0)\n",
    "\n",
    "# Function to preprocess image for LPIPS\n",
    "def preprocess_image_lpips(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Function to calculate CLIP-based metrics and LPIPS\n",
    "def calculate_metrics(image_folder, K=20):\n",
    "    image_folder = os.path.join(image_folder, \"eval_vis\")\n",
    "    embeddings = []\n",
    "    lpips_images = []\n",
    "    image_files = [os.path.join(image_folder, file) for file in os.listdir(image_folder) if (file.endswith(('png', 'jpg', 'jpeg')) and not \"ess\" in file and not \"intermediate_rewards\" in file)]\n",
    "\n",
    "    if len(image_files) == 0:\n",
    "        raise ValueError(f\"No images found in the folder: {image_folder}\")\n",
    "\n",
    "    # Preprocess images and compute embeddings\n",
    "    for image_path in tqdm(image_files):\n",
    "        try:\n",
    "            # For CLIP\n",
    "            pixel_values = preprocess_image(image_path).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                embedding = model.get_image_features(pixel_values).cpu().numpy().squeeze()\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "            # For LPIPS\n",
    "            lpips_image = preprocess_image_lpips(image_path).to(device)\n",
    "            lpips_images.append(lpips_image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "\n",
    "    if len(embeddings) == 0:\n",
    "        raise ValueError(\"No embeddings were generated. Please check your images and preprocessing steps.\")\n",
    "    \n",
    "    # ---- Calculate Mean Pairwise Distance (CLIP-based) ----\n",
    "    pairwise_distances = pdist(embeddings, metric='cosine')\n",
    "    mean_distance = np.mean(pairwise_distances)\n",
    "    num_distances = pairwise_distances.size\n",
    "    std_error = np.std(pairwise_distances) / np.sqrt(num_distances)\n",
    "    \n",
    "    # ---- Calculate Truncated CLIP Entropy (TCE) ----\n",
    "    covariance_matrix = np.cov(embeddings, rowvar=False)\n",
    "    eigenvalues = np.linalg.eigvalsh(covariance_matrix)[-K:]\n",
    "    TCE_K = (K / 2) * np.log(2 * np.pi * np.e) + (1 / 2) * np.sum(np.log(eigenvalues))\n",
    "    \n",
    "    # ---- Calculate LPIPS-based diversity ----\n",
    "    lpips_distances = []\n",
    "    num_images = len(lpips_images)\n",
    "    for i in range(num_images):\n",
    "        for j in range(i+1, num_images):\n",
    "            with torch.no_grad():\n",
    "                distance = lpips_model(lpips_images[i], lpips_images[j]).item()\n",
    "            lpips_distances.append(distance)\n",
    "    \n",
    "    mean_lpips = np.mean(lpips_distances)\n",
    "    std_lpips = np.std(lpips_distances)\n",
    "    \n",
    "    return mean_distance, std_error, TCE_K, mean_lpips, std_lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = \"logs/SMC/aesthetic/2024.09.26_01.12.19\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating images in logs/SMC/aesthetic/2024.09.26_01.12.19\n",
      "Aesthetic score:  7.056802809238434\n",
      "Aesthetic score std:  0.3690912412318431\n",
      "HPS score:  0.27003603242337704\n",
      "HPS score std:  0.008796761060747487\n",
      "Image reward score:  1.1079169440781698\n",
      "Image reward score std:  0.5541496030510243\n",
      "Pick score:  0.21461675222963095\n",
      "Pick score std:  0.00884714490436561\n",
      "Clip score:  0.25687733199447393\n",
      "Clip score std:  0.019748996170760567\n"
     ]
    }
   ],
   "source": [
    "aesthetic_score = []\n",
    "hps_score = []\n",
    "imagereward_score = []\n",
    "pick_score = []\n",
    "clip_score = []\n",
    "image_names = [file for file in os.listdir(img_folder + \"/eval_vis\") if (file.endswith(('png', 'jpg', 'jpeg')) and not \"ess\" in file and not \"intermediate_rewards\" in file)]\n",
    "for image_name in image_names:\n",
    "\n",
    "    image_path = os.path.join(img_folder + \"/eval_vis\", image_name)\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = torchvision.transforms.ToTensor()(image).unsqueeze(0).to('cuda')\n",
    "\n",
    "    prompt = image_name.split(\"|\")[0].split(\"_\")[-1][:-1]\n",
    "    # print(prompt)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        clip_score.append(clip_fn(image, prompt).item())\n",
    "        aesthetic_score.append(aesthetic_fn(image, prompt).item())\n",
    "        hps_score.append(hps_fn(image, prompt).item())\n",
    "        imagereward_score.append(imagereward(image, prompt).item())\n",
    "        pick_score.append(pick_fn(image, prompt).item())\n",
    "\n",
    "print(f\"Finished evaluating images in {img_folder}\")\n",
    "print(\"Aesthetic score: \", np.mean(aesthetic_score))\n",
    "print(\"Aesthetic score std: \", np.std(aesthetic_score))\n",
    "print(\"HPS score: \", np.mean(hps_score))\n",
    "print(\"HPS score std: \", np.std(hps_score))\n",
    "print(\"Image reward score: \", np.mean(imagereward_score))\n",
    "print(\"Image reward score std: \", np.std(imagereward_score))\n",
    "print(\"Pick score: \", np.mean(pick_score))\n",
    "print(\"Pick score std: \", np.std(pick_score))\n",
    "print(\"Clip score: \", np.mean(clip_score))\n",
    "print(\"Clip score std: \", np.std(clip_score))\n",
    "    \n",
    "# Save the results to a text file\n",
    "names = [\"Aesthetic score\", \"Aesthetic score std\", \"HPS score\", \"HPS score std\",\n",
    "         \"Image reward score\", \"Image reward score std\", \"Pick score\", \"Pick score std\", \"CLIP score\", \"CLIP score std\"]\n",
    "\n",
    "values = [np.mean(aesthetic_score), np.std(aesthetic_score),\n",
    "          np.mean(hps_score), np.std(hps_score),\n",
    "          np.mean(imagereward_score), np.std(imagereward_score),\n",
    "          np.mean(pick_score), np.std(pick_score),\n",
    "          np.mean(clip_score), np.std(clip_score)]\n",
    "\n",
    "# Format the values to 5 decimal places\n",
    "formatted_values = [f\"{v:.5f}\" for v in values]\n",
    "\n",
    "with open(os.path.join(img_folder, \"eval_results.csv\"), \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(names)\n",
    "    writer.writerow(formatted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished evaluating images in logs/SMC/aesthetic/2024.09.26_01.12.19\n",
      "Mean Pairwise Distance (CLIP-based Diversity Metric): 0.33313817014833663\n",
      "Standard Error of the Distance: 0.007745194941195248\n",
      "Truncated CLIP Entropy (TCE): -42.314139664497574\n",
      "Mean LPIPS Distance: 0.647677265604337\n",
      "Standard Deviation of LPIPS Distance: 0.04706209534226535\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "try:\n",
    "    mean_distance, std_error, TCE, mean_lpips, std_lpips = calculate_metrics(img_folder, K=20)\n",
    "    print(f\"Finished evaluating images in {img_folder}\")\n",
    "    print(f\"Mean Pairwise Distance (CLIP-based Diversity Metric): {mean_distance}\")\n",
    "    print(f\"Standard Error of the Distance: {std_error}\")\n",
    "    print(f\"Truncated CLIP Entropy (TCE): {TCE}\")\n",
    "    print(f\"Mean LPIPS Distance: {mean_lpips}\")\n",
    "    print(f\"Standard Deviation of LPIPS Distance: {std_lpips}\")\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    names = [\"Mean Pairwise Distance (CLIP)\", \"Standard Error of the Distance (CLIP)\", \n",
    "             \"Truncated CLIP Entropy (TCE)\", \"Mean LPIPS Distance\", \"Std Dev LPIPS Distance\"]\n",
    "    values = [mean_distance, std_error, TCE, mean_lpips, std_lpips]\n",
    "\n",
    "    # Format the values to 5 decimal places\n",
    "    formatted_values = [f\"{v:.5f}\" for v in values]\n",
    "\n",
    "    with open(os.path.join(img_folder, \"eval_diversity_results.csv\"), \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(names)\n",
    "        writer.writerow(formatted_values)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
